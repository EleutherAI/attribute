{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installation\n",
    "from importlib.util import find_spec\n",
    "in_colab = find_spec(\"google.colab\") is not None\n",
    "\n",
    "\n",
    "src = \"\"\"[tool.uv]\n",
    "override-dependencies = [\"\"\"\n",
    "replace_with = \"\"\"\n",
    "    \"torch ; sys_platform == 'never'\",\n",
    "    \"pytorch-triton ; sys_platform == 'never'\",\n",
    "\"\"\"\n",
    "\n",
    "if in_colab:\n",
    "    %env SPARSIFY_DISABLE_TRITON=1\n",
    "    %env OFFLOAD_TRANSCODER=1\n",
    "    %cd /content\n",
    "    !git clone https://github.com/EleutherAI/attribute\n",
    "    %cd attribute\n",
    "    !git pull\n",
    "    pyproject_text = open(\"pyproject.toml\").read()\n",
    "    if src in pyproject_text:\n",
    "        after_src = pyproject_text.index(src) + len(src)\n",
    "        current = pyproject_text[after_src:][:len(replace_with)]\n",
    "        if current != replace_with:\n",
    "            pyproject_text = pyproject_text[:after_src] + replace_with + pyproject_text[after_src:]\n",
    "            open(\"pyproject.toml\", \"w\").write(pyproject_text)\n",
    "    !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "    !uv pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results && cd results && git clone https://huggingface.co/nev/Llama-3.2-1B-mntss-skip-transcoder-cache --depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serve\n",
    "import torch\n",
    "torch._functorch.config.donated_buffer = False\n",
    "serve.default_config.batch_size = 8\n",
    "serve.main().launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
